{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Python\n",
    "\n",
    "## Simple parsing with HTMLParser\n",
    "\n",
    "In this notebook you will practice one of the workflows for using `HTMLParser` effectively. As you already know, `HTMLParser` is a streaming parser, where data comes in with chunks. Each chunk of data has delimeters like tags. \n",
    "\n",
    "It might feel a bit complicated to have special methods to look at tags, and others to process data - this is one of the caveats of using a streaming parser.\n",
    "\n",
    "For this exercise, you will use predefined HTML variables with raw content that can be parsed. Instead of requesting the data from the web, the content is already defined and available to be processed. The process is the same to scrape the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\"/>\n",
    "<title>1992 World Junior Championships in Athletics – Men's high jump - Wikipedia</title>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is available, import the html modules so that you can write the class next. The class has to have the `__init__()` method and set some class attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class Parser(HTMLParser):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.recording = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"title\":\n",
    "            self.recording = True\n",
    "        else:\n",
    "            self.recording = False\n",
    "            \n",
    "    def handle_data(self, data):\n",
    "        if self.recording:\n",
    "            print(f\"Found data for tag: {data}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data for tag: 1992 World Junior Championships in Athletics – Men's high jump - Wikipedia\n",
      "Found data for tag: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = Parser()\n",
    "p.feed(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is `handled_data()` printing twice? The second line appears to have an _empty_ data. Here is one way to find out: update the `handle_data()` method so that it displays the string with the `repr()` built-in function:\n",
    "\n",
    "```python\n",
    "    def handle_data(self, data):\n",
    "        if self.recording:\n",
    "            print(f\"Found data for tag: {repr(data)}\")\n",
    "```\n",
    "\n",
    "Run the cell where the class lives and re-run the Parser cell again to see if you spot the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A string with an empty string var wouldn't show the variable: \n",
      "A string with an empty string var wouldn't show the variable: ''\n"
     ]
    }
   ],
   "source": [
    "# repr() helps when there are hidden characters that `print()` wouldn't show. \n",
    "empty = \"\"\n",
    "print(f\"A string with an empty string var wouldn't show the variable: {empty}\")\n",
    "print(f\"A string with an empty string var wouldn't show the variable: {repr(empty)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what changes could you make to prevent two lines showing in the output. There are several approaches you could take to improve the quality of the data gathering, and the previous cells showed one. But what if you are also dealing with newline characters? Or other non-visible characters? An alternative you could try is to append the data found to a list instead of printing, and when the parsing is completed, joining the data found.\n",
    "\n",
    "Here is how that would look with an example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992 World Junior Championships in Athletics – Men's high jump\n",
      "\n",
      "Wikipedia\n"
     ]
    }
   ],
   "source": [
    "captured_data = [\"1992 World Junior Championships in Athletics – Men's high jump\", \"\\n\", \"\\n\", \"Wikipedia\"]\n",
    "print(''.join(captured_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scrapy and XPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# create a virtual environment\n",
    "python3 -m venv venv\n",
    "\n",
    "# activate the venv\n",
    "source venv/bin/activate\n",
    "\n",
    "# install scrapy\n",
    "pip install scrapy\n",
    "\n",
    "# start a new project\n",
    "scrapy startproject vulnerabilities\n",
    "\n",
    "# enter the new project directory\n",
    "cd vulnerabilities\n",
    "\n",
    "# genspider needs two arguments: name of the spider and the domain\n",
    "scrapy genspider cve cve.mitre.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Data with XPath and Scrapy Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scrapy shell http://cve.mitre.org/data/refs/refmap/source-EXPLOIT-DB.html\n",
    "\n",
    "response.url\n",
    "\n",
    "response.css\n",
    "\n",
    "response.xpath('//table')\n",
    "\n",
    "len(response.xpath('//table'))\n",
    "\n",
    "response.css('table')\n",
    "\n",
    "len(response.css('table'))\n",
    "\n",
    "len(response.xpath('//table').xpath('tr'))\n",
    "\n",
    "response.xpath('//table')[0]\n",
    "\n",
    "len(response.xpath('//table')[0].xpath('tr'))\n",
    "\n",
    "len(response.xpath('//table')[3].xpath('tr'))\n",
    "\n",
    "table = response.xpath('//table')[3]\n",
    "\n",
    "len(table.xpath('tr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "table.xpath('//tr') # finds every single row\n",
    "\n",
    "child = table.xpath('//tr')[10]\n",
    "\n",
    "child.xpath('td//text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "child.xpath('td//text()')[0].extract()\n",
    "\n",
    "for row in table.xpath('//tr'):\n",
    "    try:\n",
    "        print(row.xpath('td//text()')[0].extract())\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scrapy Spider for Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "vim cve.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class CveSpider(scrapy.Spider):\n",
    "    name = 'cve'\n",
    "    allowed_domains = ['cve.mitre.org']\n",
    "    start_urls = ['http://cve.mitre.org/data/refs/refmap/source-EXPLOIT-DB.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for child in response.xpath('//table'):\n",
    "            if len(child.xpath('tr')) > 100:\n",
    "                table = child\n",
    "                break\n",
    "        for row in table.xpath('//tr'):\n",
    "            try:\n",
    "                print(row.xpath('td//text()')[0].extract())\n",
    "            except IndexError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scrapy crawl cve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence and Efficiency with Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class CveSpider(scrapy.Spider):\n",
    "    name = 'cve'\n",
    "    allowed_domains = ['cve.mitre.org']\n",
    "    # download the html\n",
    "    start_urls = ['http://cve.mitre.org/data/refs/refmap/source-EXPLOIT-DB.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for child in response.xpath('//table'):\n",
    "            if len(child.xpath('tr')) > 100:\n",
    "                table = child\n",
    "                break\n",
    "        for row in table.xpath('//tr'):\n",
    "            try:\n",
    "                print(row.xpath('td//text()')[0].extract())\n",
    "            except IndexError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting data in CSV and JSON formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "from os.path import dirname\n",
    "import csv\n",
    "import json\n",
    "\n",
    "current_dir = os.path.dirname(__file__)\n",
    "url = os.path.join(current_dir, 'source-EXPLOIT-DB.html')\n",
    "\n",
    "\n",
    "class CveSpider(scrapy.Spider):\n",
    "    name = 'cve4'\n",
    "    allowed_domains = ['cve.mitre.org']\n",
    "    # start_urls = ['http://cve.mitre.org/data/refs/refmap/source-EXPLOIT-DB.html']\n",
    "    start_urls = [f\"file://{url}\"]\n",
    "    def parse(self, response):\n",
    "        for child in response.xpath('//table'):\n",
    "            if len(child.xpath('tr')) > 100:\n",
    "                table = child\n",
    "                break\n",
    "        count = 0\n",
    "        data = {}\n",
    "\n",
    "        json_file = open('vulnerabilities.json', 'w')\n",
    "                \n",
    "        for row in table.xpath('//tr'):\n",
    "            if count > 100:\n",
    "                break\n",
    "            try:\n",
    "                exploit_id = row.xpath('td//text()')[0].extract()\n",
    "                cve_id = row.xpath('td//text()')[2].extract()\n",
    "                data[exploit_id] = cve_id\n",
    "                count += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "        json.dump(data, json_file)\n",
    "        json_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting data to a SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "from os.path import dirname\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "current_dir = os.path.dirname(__file__)\n",
    "url = os.path.join(current_dir, 'source-EXPLOIT-DB.html')\n",
    "\n",
    "\n",
    "class CveSpider(scrapy.Spider):\n",
    "    name = 'cve5'\n",
    "    allowed_domains = ['cve.mitre.org']\n",
    "    # start_urls = ['http://cve.mitre.org/data/refs/refmap/source-EXPLOIT-DB.html']\n",
    "    start_urls = [f\"file://{url}\"]\n",
    "    def parse(self, response):\n",
    "        connection = sqlite3.connect('vuln.db')\n",
    "        table = 'CREATE TABLE vulns (exploit TEXT, cve TEXT)'\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(table)\n",
    "        connection.commit()\n",
    "\n",
    "        for child in response.xpath('//table'):\n",
    "            if len(child.xpath('tr')) > 100:\n",
    "                table = child\n",
    "                break\n",
    "        count = 0\n",
    "        data = {}\n",
    "\n",
    "        json_file = open('vulnerabilities.json', 'w')\n",
    "                \n",
    "        for row in table.xpath('//tr'):\n",
    "            if count > 100:\n",
    "                break\n",
    "            try:\n",
    "                exploit_id = row.xpath('td//text()')[0].extract()\n",
    "                cve_id = row.xpath('td//text()')[2].extract()\n",
    "                cursor.execute('INSERT INTO vulns (exploit, cve) VALUES(?, ?)'), (exploit_id, cve_id)\n",
    "                connection.commit()\n",
    "                count += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "        json.dump(data, json_file)\n",
    "        json_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
